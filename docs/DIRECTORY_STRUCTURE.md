# ML Resume Analyzer - Directory Structure (Backbone)

```
resume-analyzer/
├── README.md
├── requirements.txt
├── app.py                          # Gradio interface
├── config.py                       # Configuration settings
│
├── src/
│   ├── __init__.py
│   │
│   ├── models/                     # NEW - ML Python code
│   │   ├── __init__.py
│   │   ├── data_generator.py       # Synthetic training data generator
│   │   ├── classification.py       # Fine-tuned embeddings training
│   │   ├── scoring.py             # Neural scoring models training
│   │   └── model_utils.py         # Model loading/saving utilities
│   │
│   ├── parser/                     # EXISTING - Text processing
│   │   ├── __init__.py
│   │   ├── pdf_extractor.py
│   │   ├── text_processor.py
│   │   └── section_parser.py
│   │
│   ├── database/                   # EXISTING - Analysis engines
│   │   ├── __init__.py
│   │   ├── job_analyzer.py
│   │   └── vector_store.py         # MAJOR REFACTOR - Use ML models
│   │
│   └── utils/                      # Utilities
│       ├── __init__.py
│       └── logging_config.py
│
├── data/
│   ├── training/                   # Training datasets
│   │   └── synthetic_dataset.json  # Generated by data_generator.py
│   │
│   ├── sample_resumes/            # EXISTING test PDFs
│   │   ├── standard_1pg_resume.pdf
│   │   ├── long_resume_6pgs.pdf
│   │   └── sparse_resume.pdf
│   │
│   └── sample_jobs/               # Test job descriptions
│       ├── software_engineer.txt
│       ├── data_scientist.txt
│       └── devops_engineer.txt
│
├── models/                        # Trained model artifacts (created after training)
│   ├── embeddings/                # Fine-tuned sentence transformers
│   └── scorers/                   # Trained PyTorch models (.pth files)
│
├── tests/
│   ├── __init__.py
│   ├── test_models/               # NEW - Test ML components
│   │   ├── test_data_generator.py
│   │   ├── test_classification.py
│   │   └── test_scoring.py
│   │
│   └── test_parser/              # EXISTING tests
│       └── test_section_parser.py
│
├── scripts/                      # Training automation
│   ├── train_models.py           # Run full training pipeline
│   └── generate_data.py          # Generate training dataset
│
└── vector_db/                    # ChromaDB storage (auto-created)
```

## Essential Python Files to Create:

### NEW ML Components:
1. `src/models/__init__.py`
2. `src/models/data_generator.py` (already created)
3. `src/models/classification.py` (to create)
4. `src/models/scoring.py` (to create) 
5. `src/models/model_utils.py` (to create)

### NEW Scripts:
6. `scripts/train_models.py` (training automation)
7. `scripts/generate_data.py` (data generation script)

### NEW Configuration:
8. `config.py` (centralized settings)
9. `app.py` (Gradio interface)
10. `src/utils/logging_config.py` (logging setup)

### EXISTING Files (keep as-is):
- All current parser and database files from prototype
- All current test files from prototype 

## Updated requirements.txt:
```txt
# Existing
PyMuPDF
chromadb
sentence-transformers
numpy

# NEW ML dependencies
torch>=2.0.0
scikit-learn>=1.3.0
transformers>=4.30.0
gradio>=3.40.0
pandas>=2.0.0
matplotlib>=3.7.0
```
